{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDmRRLUAwHp2",
        "outputId": "8a6e997b-61dc-4bd5-9ca4-ea2e2d47c9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Gaussian Naive Bayes\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[108  40]\n",
            " [ 50 122]]\n",
            "Succes Rate:  71.96495619524404\n",
            "     Logistic Regression\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[112  36]\n",
            " [ 42 130]]\n",
            "Succes Rate:  76.34543178973716\n",
            "     Logistic Regression\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[115  33]\n",
            " [ 42 130]]\n",
            "Succes Rate:  76.65832290362953\n",
            "K-Nearest Neighbors\n",
            "[[ 98  50]\n",
            " [ 42 130]]\n",
            "Succes Rate:  71.33917396745932\n",
            "K-Nearest Neighbors\n",
            "[[107  41]\n",
            " [ 33 139]]\n",
            "Succes Rate:  76.97121401752189\n",
            "Decision Tree Classifier\n",
            "           quality\n",
            "   3   4   5   6   7   8\n",
            "[[113  35]\n",
            " [ 50 122]]\n",
            "Succes Rate:  73.52941176470587\n",
            "Decision Tree Classifier\n",
            "           quality\n",
            "   3   4   5   6   7   8\n",
            "[[117  31]\n",
            " [ 50 122]]\n",
            "Succes Rate:  74.78097622027533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Classifier\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[110  38]\n",
            " [ 41 131]]\n",
            "Succes Rate:  75.40675844806007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support Vector Classifier\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[114  34]\n",
            " [ 42 130]]\n",
            "Succes Rate:  76.34543178973716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6d1e6bb8fa80>:259: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  rfc.fit(X_train,y_train) #Train\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Classifier\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[122  26]\n",
            " [ 37 135]]\n",
            "Succes Rate:  80.72590738423028\n",
            "Random Forest Classifier\n",
            "           quality\n",
            "    3   4   5   6   7   8\n",
            "[[124  24]\n",
            " [ 37 135]]\n",
            "Succes Rate:  81.03879849812265\n",
            "votingggggg\n",
            "Model:knn - Accuracy:76.88%"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6d1e6bb8fa80>:276: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  rfc.fit(X_train,y_train) #Train\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neighbors/_classification.py:215: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  return self._fit(X, y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model:SVC - Accuracy:76.25%\n",
            "Model:rf - Accuracy:80.94%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-6d1e6bb8fa80>:302: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  clf.fit(X_train, y_train)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hard voting: 77.50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:99: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_label.py:134: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, dtype=self.classes_.dtype, warn=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "soft voting: 77.81\n"
          ]
        }
      ],
      "source": [
        "#1.libraries\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#2. Data reading\n",
        "data = pd.read_csv(\"red-wine.csv\")\n",
        "\n",
        "data.loc[data['quality'] <= 5, 'quality'] = 0\n",
        "data.loc[data['quality'] > 5, 'quality'] = 1\n",
        "\n",
        "\n",
        "#Division of data into features and quality\n",
        "data_x = data.iloc[:,0:11]\n",
        "data_y = data.iloc[:,11:12]\n",
        "\n",
        "\n",
        "#Division of data into education and testing\n",
        "x_train, x_test,y_train,y_test = train_test_split(data_x,data_y ,test_size=0.2, random_state=0)\n",
        "\n",
        "\n",
        "#Data creation\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(x_train)\n",
        "X_test = sc.fit_transform(x_test)\n",
        "Y_train = sc.fit_transform(y_train)\n",
        "'''\n",
        "#headings of features are written\n",
        "features = [\"fixed acidity\",\"volatile acidity\",\"citric acid\",\"residual sugar\",\"chlorides\",\"free sulfur dioxide\",\"total sulfur dioxide\",\"density\",\"pH\",\"sulphates\",\"alcohol\"]\n",
        "\n",
        "# Separating out the features\n",
        "x = data.loc[:, features].values\n",
        "\n",
        "# Separating out the target\n",
        "y = data.loc[:,['quality']].values\n",
        "\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "#two-dimensional drawing of data set\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "principalComponents = pca.fit_transform(x)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2'])\n",
        "\n",
        "\n",
        "\n",
        "finalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n",
        "\n",
        "fig = plt.figure(figsize = (12,12))\n",
        "ax = fig.add_subplot(1,1,1)\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_title('2 component PCA', fontsize = 20)\n",
        "targets = [0,1]\n",
        "colors = [\"blue\" , \"red\" ]\n",
        "for quality, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['quality'] == quality\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , c = color\n",
        "               , s = 50)\n",
        "ax.legend(targets)\n",
        "ax.grid()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "#three-dimensional drawing of data set\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=3)\n",
        "principalComponents = pca.fit_transform(x)\n",
        "principalDf = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2',\"principal component 3\"])\n",
        "\n",
        "\n",
        "\n",
        "finalDf = pd.concat([principalDf, data[['quality']]], axis = 1)\n",
        "\n",
        "fig = plt.figure(figsize = (12,12))\n",
        "ax = fig.add_subplot(111 , projection='3d')\n",
        "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
        "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
        "ax.set_zlabel(\"principal component 3\", fontsize = 15)\n",
        "ax.set_title('3 component PCA', fontsize = 20)\n",
        "targets = [0,1]\n",
        "colors = [ 'blue' , \"red\" ]\n",
        "for quality, color in zip(targets,colors):\n",
        "    indicesToKeep = finalDf['quality'] == quality\n",
        "    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 2']\n",
        "               , finalDf.loc[indicesToKeep, 'principal component 3']\n",
        "                  ,marker = \"o\", c = color\n",
        "               , s = 20)\n",
        "ax.legend(targets)\n",
        "ax.grid()\n",
        "'''\n",
        "#Application of NAive Bayes algorithm and evaluation with confusion_matrix\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB(priors=None)\n",
        "\n",
        "gnb.fit(X_train, y_train) #Train\n",
        "\n",
        "pred_gnb = gnb.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_gnb)\n",
        "print('     Gaussian Naive Bayes')\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((108+122)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "\n",
        "\n",
        "#Application of Logistic Regression algorithm and evaluation with confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logr = LogisticRegression(random_state=None)\n",
        "\n",
        "logr.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_logr = logr.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_logr)\n",
        "print('     Logistic Regression')\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((114+130)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "#Application of Logistic Regression algorithm and evaluation with confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logr = LogisticRegression(random_state=None ,multi_class = \"multinomial\" ,solver = \"lbfgs\" ,C =1 ,tol = 2 ,max_iter =100 )\n",
        "\n",
        "logr.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_logr = logr.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_logr)\n",
        "print('     Logistic Regression')\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((115+130)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "#Implementation of K-Nearest Neighbors algorithm and evaluation with confusion_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_knn = knn.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_knn)\n",
        "print(\"K-Nearest Neighbors\")\n",
        "print(cm)\n",
        "s = (((98+130)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=100 ,  weights = \"distance\" , algorithm=\"kd_tree\"\n",
        "                           , leaf_size=75 , p=4.5 , metric=\"minkowski\")\n",
        "knn.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_knn = knn.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_knn)\n",
        "print(\"K-Nearest Neighbors\")\n",
        "print(cm)\n",
        "s = (((107+139)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "\n",
        "\n",
        "#Decision tree algorithm implementation and evaluation with confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(random_state=0)\n",
        "\n",
        "dtc.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_dtc = dtc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_dtc)\n",
        "print('Decision Tree Classifier')\n",
        "print(\"           quality\")\n",
        "print(\"   3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((113+122)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "#Decision tree algorithm implementation and evaluation with confusion_matrix\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dtc = DecisionTreeClassifier(random_state=0 , criterion=\"gini\" , splitter=\"best\" , min_samples_leaf=4)\n",
        "\n",
        "dtc.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_dtc = dtc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_dtc)\n",
        "print('Decision Tree Classifier')\n",
        "print(\"           quality\")\n",
        "print(\"   3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((117+122)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "\n",
        "#Application of Support Vector Classifier algorithm and evaluation with confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "rbf_svc = SVC(random_state=0 , probability = True )\n",
        "rbf_svc.fit(X_train, y_train) #Train\n",
        "\n",
        "pred_svc = rbf_svc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test, pred_svc)\n",
        "print(\"Support Vector Classifier\")\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((110+131)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "#Application of Support Vector Classifier algorithm and evaluation with confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "rbf_svc = SVC(random_state=0 , probability = True , kernel=\"rbf\" , C=0.3)\n",
        "rbf_svc.fit(X_train, y_train) #Train\n",
        "\n",
        "pred_svc = rbf_svc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test, pred_svc)\n",
        "print(\"Support Vector Classifier\")\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((114+130)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "\n",
        "#Implementation of Random Forest algorithm and evaluation with confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier( random_state=0 )\n",
        "\n",
        "rfc.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_rfc = rfc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_rfc)\n",
        "print('Random Forest Classifier')\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((124+134)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "#Implementation of Random Forest algorithm and evaluation with confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rfc = RandomForestClassifier( random_state=0 ,n_estimators=12 , min_samples_split=2 )\n",
        "\n",
        "rfc.fit(X_train,y_train) #Train\n",
        "\n",
        "pred_rfc = rfc.predict(X_test) #Predict\n",
        "\n",
        "#Confusion_matrix\n",
        "cm = confusion_matrix(y_test,pred_rfc)\n",
        "print('Random Forest Classifier')\n",
        "print(\"           quality\")\n",
        "print(\"    3\" , \"  4\" , \"  5\" , \"  6\" , \"  7\" , \"  8\")\n",
        "print(cm)\n",
        "s = (((124+135)/(1598*0.2))*100)\n",
        "print(\"Succes Rate: \",s)\n",
        "\n",
        "\n",
        "\n",
        "print(\"votingggggg\")\n",
        "\n",
        "clf2 = KNeighborsClassifier(n_neighbors=100 ,  weights = \"distance\" , algorithm=\"kd_tree\"\n",
        "                           , leaf_size=75 , p=4.5 , metric=\"minkowski\")\n",
        "clf3 = SVC(random_state=0 , probability = True , kernel=\"rbf\" , C=0.3)\n",
        "clf4 = RandomForestClassifier( random_state=0 ,n_estimators=12 , min_samples_split=2 )\n",
        "\n",
        "clfs = [('knn', clf2),('SVC', clf3),(\"rf\" , clf4) ]\n",
        "\n",
        "for clf_tuple in clfs:\n",
        "    clf_name, clf = clf_tuple\n",
        "    clf.fit(X_train, y_train)\n",
        "    acc = clf.score(X_test, y_test)\n",
        "    print('Model:{} - Accuracy:{:.2f}%'.format(clf_name, acc*100))\n",
        "\n",
        "\n",
        "hard_clf = VotingClassifier(estimators=clfs, voting='hard' )\n",
        "\n",
        "hard_clf.fit(X_train, y_train)\n",
        "\n",
        "predict_proba = hard_clf.predict(X_test)\n",
        "\n",
        "print('hard voting: {:.2f}'.format(hard_clf.score(X_test, y_test)*100))\n",
        "\n",
        "\n",
        "soft_clf = VotingClassifier(estimators=clfs, voting='soft'  )\n",
        "soft_clf.fit(X_train, y_train)\n",
        "predict_proba = hard_clf.predict(X_test)\n",
        "print('soft voting: {:.2f}'.format(soft_clf.score(X_test, y_test)*100))\n"
      ]
    }
  ]
}